import torch
import numpy as np
from tqdm import tqdm
from copy import deepcopy

# ============================== UTILS FOR UAT ============================== #

def hotflip_attack(averaged_grad, embedding_matrix, trigger_token_ids,
                   increase_loss=False, num_candidates=1):
    """
    The "Hotflip" attack described in Equation (2) of the paper. This code is heavily inspired by
    the nice code of Paul Michel here https://github.com/pmichel31415/translate/blob/paul/
    pytorch_translate/research/adversarial/adversaries/brute_force_adversary.py

    This function takes in the model's average_grad over a batch of examples, the model's
    token embedding matrix, and the current trigger token IDs. It returns the top token
    candidates for each position.

    If increase_loss=True, then the attack reverses the sign of the gradient and tries to increase
    the loss (decrease the model's probability of the true class). For targeted attacks, you want
    to decrease the loss of the target class (increase_loss=False).
    """
    averaged_grad = averaged_grad
    embedding_matrix = embedding_matrix
    trigger_token_embeds = torch.nn.functional.embedding(torch.LongTensor(trigger_token_ids).cuda(),
                                                         embedding_matrix).detach().unsqueeze(0)
    averaged_grad = averaged_grad.unsqueeze(0)

    # einsum does not support half precision, so we rewrite as bmm.
    gradient_dot_embedding_matrix = torch.bmm(averaged_grad, torch.transpose(embedding_matrix, 0, 1).unsqueeze(0))
    
    if not increase_loss:
        gradient_dot_embedding_matrix *= -1    # lower versus increase the class probability.
    if num_candidates > 1: # get top k options
        _, best_k_ids = torch.topk(gradient_dot_embedding_matrix, num_candidates, dim=2)
        return best_k_ids.detach().cpu().numpy()[0]
    _, best_at_each_step = gradient_dot_embedding_matrix.max(2)
    return best_at_each_step[0].detach().cpu().numpy()

def random_attack(embedding_matrix, trigger_token_ids, num_candidates=1):
    """
    Randomly search over the vocabulary. Gets num_candidates random samples and returns all of them.
    """
    embedding_matrix = embedding_matrix.cpu()
    new_trigger_token_ids = [[None]*num_candidates for _ in range(len(trigger_token_ids))]
    for trigger_token_id in range(len(trigger_token_ids)):
        for candidate_number in range(num_candidates):
            # rand token in the embedding matrix
            rand_token = np.random.randint(embedding_matrix.shape[0])
            new_trigger_token_ids[trigger_token_id][candidate_number] = rand_token
    return new_trigger_token_ids

def nearest_neighbor_grad(averaged_grad, embedding_matrix, trigger_token_ids,
                          tree, step_size, increase_loss=False, num_candidates=1):
    """
    Takes a small step in the direction of the averaged_grad and finds the nearest
    vector in the embedding matrix using a kd-tree.
    """
    new_trigger_token_ids = [[None]*num_candidates for _ in range(len(trigger_token_ids))]
    averaged_grad = averaged_grad.cpu()
    embedding_matrix = embedding_matrix.cpu()
    if increase_loss: # reverse the sign
        step_size *= -1
    for token_pos, trigger_token_id in enumerate(trigger_token_ids):
        # take a step in the direction of the gradient
        trigger_token_embed = torch.nn.functional.embedding(torch.LongTensor([trigger_token_id]),
                                                            embedding_matrix).detach().cpu().numpy()[0]
        stepped_trigger_token_embed = trigger_token_embed + \
            averaged_grad[token_pos].detach().cpu().numpy() * step_size
        # look in the k-d tree for the nearest embedding
        _, neighbors = tree.query([stepped_trigger_token_embed], k=num_candidates)
        for candidate_number, neighbor in enumerate(neighbors[0]):
            new_trigger_token_ids[token_pos][candidate_number] = neighbor
    return new_trigger_token_ids

def get_embedding_weight(language_model):
    """
    Creates the batch of target texts with -1 placed at the end of the sequences for padding (for masking out the loss)
    """
    for module in language_model.modules():
        if isinstance(module, torch.nn.Embedding):
            if module.weight.shape[0] == language_model.gpt_neox.embed_in.weight.shape[0]: # only add a hook to wordpiece embeddings, not position embeddings
                return module.weight.detach()

def make_target_batch(tokenizer,target_texts):
    """
    Creates the batch of target texts with -1 placed at the end of the sequences for padding (for masking out the loss)
    """
    # encode items and get the max length
    encoded_texts = []
    max_len = 0
    for target_text in target_texts:
        encoded_target_text = tokenizer.encode(target_text)
        encoded_texts.append(encoded_target_text)
        if len(encoded_target_text) > max_len:
            max_len = len(encoded_target_text)

    # pad tokens, i.e., append -1 to the end of the non-longest ones
    for indx, encoded_text in enumerate(encoded_texts):
        if len(encoded_text) < max_len:
            encoded_texts[indx].extend([-1] * (max_len - len(encoded_text)))

    # convert to tensors and batch them up
    target_tokens_batch = None
    for encoded_text in encoded_texts:
        target_tokens = torch.tensor(encoded_text, device='cuda', dtype=torch.long).unsqueeze(0)
        if target_tokens_batch is None:
            target_tokens_batch = target_tokens
        else:
            target_tokens_batch = torch.cat((target_tokens, target_tokens_batch), dim=0)
    return target_tokens_batch

# Gets the loss of the target_tokens using the triggers as the context
def get_loss(language_model, batch_size, trigger, target, device='cuda'):
    # context is trigger repeated batch size
    tensor_trigger = torch.tensor(trigger, device=device, dtype=torch.long).unsqueeze(0).repeat(batch_size, 1)
    mask_out = -1 * torch.ones_like(tensor_trigger) # we zero out the loss for the trigger tokens
    lm_input = torch.cat((tensor_trigger, target), dim=1) # we feed the model the trigger + target texts
    mask_and_target = torch.cat((mask_out, target), dim=1) # has -1's + target texts for loss computation
    lm_input[lm_input == -1] = 1   # put random token of 1 at end of context (its masked out)
    #loss = language_model(lm_input, labels=mask_and_target)[0]
    criterion = torch.nn.NLLLoss(ignore_index=-1)
    output = torch.log_softmax(language_model(lm_input)[0], dim=-1)
    #output = language_model(lm_input)[0]
    loss = criterion(output.view(-1, output.size(-1)), mask_and_target.view(-1))
    return loss

# ============================== UAT CLASS DEFINITION ============================== #

class UAT():
    def __init__(self):
        super().__init__()
        print("starting UAT....")
        self.extracted_grads = []
    
    def extract_grad_hook(self, module, grad_in, grad_out):
        self.extracted_grads.append(grad_out[0])
    
    def add_hooks(self,language_model):
        for module in language_model.modules():
            if isinstance(module, torch.nn.Embedding):
                if module.weight.shape[0] == language_model.gpt_neox.embed_in.weight.shape[0]:
                    module.weight.requires_grad = True
                    module.register_backward_hook(self.extract_grad_hook)
    
    def predict(self, targets, tokenizer, model, num_generate=20, num_optim_tokens=8,
                num_steps=10, verbose=False,start=None):
        """
        Generate predicted triggers for the provided targets

        :param num_generate: number of triggers to generate for each target
        """
        predictions = {}
        for i, target in tqdm(list(enumerate(targets))):
            if verbose:
                print(f"Generating triggers for target {i+1}/{len(targets)}: {target}")
            predictions[target] = self._predict_single_target(target, tokenizer, model, num_generate, num_optim_tokens,
                                                              num_steps, verbose)
        return predictions

    def _predict_single_target(self, target, tokenizer, model, num_generate, num_optim_tokens,
                               num_steps, verbose):
        """
        Generate predicted trigger for the provided target
        """
        self.add_hooks(model)
        target_tokens = make_target_batch(tokenizer,[target])
        embedding_weight = get_embedding_weight(model) # save the word embedding matrix
        optim_prompts = []
        # Warning. the below contains extremely offensive content.
        # Create a batch of targets you'd like to increase the likelihood of.
        # This can be modified to include whatever topic you want
        # (e.g., sports, technology, hate speech, etc.)
        # batch and pad the target tokens

        # to save time, we do not use random restarts; to add these back in, wrap the below in a for loop
        for _ in range(num_generate):
            if verbose:
                print(f"Generating trigger {_+1}/{num_generate}")
            #
            total_vocab_size = model.gpt_neox.embed_in.weight.shape[0]  # total number of subword pieces in the GPT-2 model
            trigger_token_length = num_optim_tokens  # how many subword pieces in the trigger
            batch_size_loss = target_tokens.shape[0]

            # sample random initial trigger
            trigger_tokens = np.random.randint(total_vocab_size, size=trigger_token_length)

            # get initial loss for the trigger
            model.zero_grad()
            loss = get_loss(model, batch_size_loss, trigger_tokens, target_tokens)
            best_loss = loss
            counter = 0
            end_iter = False

            for i in range(num_steps):  # this many updates of the entire trigger sequence
                for token_to_flip in range(0, trigger_token_length): # for each token in the trigger
                    if end_iter:  # no loss improvement over whole sweep -> continue to new random restart
                        continue

                    # Get average gradient w.r.t. the triggers
                    extracted_grads = [] # clear the gradient from past iterations
                    self.extracted_grads = []
                    loss.backward()
                    #averaged_grad = torch.sum(extracted_grads[0], dim=0)
                    averaged_grad = torch.sum(self.extracted_grads[0], dim=0)
                    averaged_grad = averaged_grad[token_to_flip].unsqueeze(0)

                    # Use hotflip (linear approximation) attack to get the top num_candidates
                    candidates = hotflip_attack(averaged_grad, embedding_weight,
                                                        [trigger_tokens[token_to_flip]], 
                                                        increase_loss=True, num_candidates=3)[0]

                    # try all the candidates and pick the best
                    curr_best_loss = 999999
                    curr_best_trigger_tokens = None
                    for cand in candidates:
                        # replace one token with new candidate
                        candidate_trigger_tokens = deepcopy(trigger_tokens)
                        candidate_trigger_tokens[token_to_flip] = cand

                        # get loss, update current best if its lower loss
                        curr_loss = get_loss(model, batch_size_loss, candidate_trigger_tokens,
                                            target_tokens)
                        if curr_loss < curr_best_loss:
                            curr_best_loss = curr_loss
                            curr_best_trigger_tokens = deepcopy(candidate_trigger_tokens)

                    # Update overall best if the best current candidate is better
                    if curr_best_loss < best_loss:
                        counter = 0 # used to exit early if no improvements in the trigger
                        best_loss = curr_best_loss
                        trigger_tokens = deepcopy(curr_best_trigger_tokens)
                        #print("Loss: " + str(best_loss.data.item()))
                        #print(tokenizer.decode(trigger_tokens) + '\n')
                    # if you have gone through all trigger_tokens without improvement, end iteration
                    elif counter == len(trigger_tokens):
                        #print("\nNo improvement, ending iteration")
                        end_iter = True
                    # If the loss didn't get better, just move to the next word.
                    else:
                        counter = counter + 1

                    # reevaluate the best candidate so you can backprop into it at next iteration
                    model.zero_grad()
                    loss = get_loss(model, batch_size_loss, trigger_tokens, target_tokens)
                if verbose:
                    if i % 10 == 0:
                        print('{} {:.3f}'.format(i, best_loss.data.item()))
            optim_prompts.append(tokenizer.decode(trigger_tokens))
            if verbose:
                print('{} {:.3f}'.format(i, best_loss.data.item()))

        # ========== detokenize and print the optimized prompt ========== #
        if verbose:
            print('target_text:', target)
            for i, p in enumerate(optim_prompts):
                print(f'optim_prompt {i}:', p)
        
        return optim_prompts
